#!/usr/bin/env python

# Author: Tanay Agarwal (tagarwa2)

import optparse
import sys
from collections import defaultdict

optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--num_iterations", dest="num_iters", default=10, type="int", help="Number of iterations to run EM (default=10)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

# MODEL 1 DIRECTION 1

sys.stderr.write("Initializing estimates with IBM Model 1 in direction 1............\n")
# IBM Model 1 pseudocode obtained from http://mt-class.org/jhu/slides/lecture-ibm-model1.pdf

bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]
t_fe = {}
for (n, (f, e)) in enumerate(bitext):
  for f_i in set(f):
    for e_j in set(e):
      t_fe[(f_i, e_j)] = 0.25 # uniform initialization

for iteration in range(0, opts.num_iters):
  sys.stderr.write("Iteration " + str(iteration) + "\n")
  sys.stderr.write("Initializing counts\n")
  # initialize counts and totals
  count_fe = {}
  total_e = {}
  for (f_i, e_j) in t_fe.keys():
    count_fe[(f_i, e_j)] = 0
    total_e[e_j] = 0

  for (n, (f, e)) in enumerate(bitext):
    # sys.stderr.write("Sentence " + str(n) + "\n")
    # compute normalization
    s_total = {}
    for f_i in set(f):
      s_total[f_i] = 0
      for e_j in set(e):
        s_total[f_i] += t_fe[(f_i, e_j)]

    # collect counts
    for f_i in set(f):
      for e_j in set(e):
        count_fe[(f_i, e_j)] += ( t_fe[(f_i, e_j)] / s_total[f_i] )
        total_e[e_j] += ( t_fe[(f_i, e_j)] / s_total[f_i] )

  # estimate probabilities
  sys.stderr.write("Estimating probabilities\n")
  for (f_i, e_j) in t_fe.keys():
    t_fe[(f_i, e_j)] = count_fe[(f_i, e_j)] / total_e[e_j]

# MODEL 2 DIRECTION 1

sys.stderr.write("Training model with IBM Model 2 in direction 1............\n")
# IBM Model 2 pseudocode obtained from http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf

q_jilm = {}
for (n, (f, e)) in enumerate(bitext):
  for (i, f_i) in enumerate(f):
    for (j, e_j) in enumerate(e):
      l = len(e)
      m = len(f)
      q_jilm[(j, i, l, m)] = 0.25 # uniform initialization

for iteration in range(0, opts.num_iters):
  sys.stderr.write("Iteration " + str(iteration) + "\n")
  sys.stderr.write("Initializing counts\n")
  # initialize counts and totals
  count_fe = {}
  count_e = {}
  count_jilm = {}
  count_ilm = {}
  for (f_i, e_j) in t_fe.keys():
    count_fe[(f_i, e_j)] = 0
    count_e[e_j] = 0
  for (j, i, l, m) in q_jilm.keys():
    count_jilm[(j, i, l, m)] = 0
    count_ilm[(i, l, m)] = 0

  for (n, (f, e)) in enumerate(bitext):
    # sys.stderr.write("Sentence " + str(n) + "\n")
    # compute normalization
    s_total = {}
    for (i, f_i) in enumerate(f):
      s_total[i] = 0
      for (j, e_j) in enumerate(e):
        s_total[i] += ( q_jilm[(j, i, len(e), len(f))] * t_fe[(f_i, e_j)] )

    # collect counts
    for (i, f_i) in enumerate(f):
      for (j, e_j) in enumerate(e):
        delta = ( q_jilm[(j, i, len(e), len(f))] * t_fe[(f_i, e_j)] ) / s_total[i]
        count_fe[(f_i, e_j)] += delta
        count_e[e_j] += delta
        count_jilm[(j, i, len(e), len(f))] += delta
        count_ilm[(i, len(e), len(f))] += delta

  # estimate probabilities
  sys.stderr.write("Estimating probabilities\n")
  for (f_i, e_j) in t_fe.keys():
    t_fe[(f_i, e_j)] = count_fe[(f_i, e_j)] / count_e[e_j]
  for (j, i, l, m) in q_jilm.keys():
    q_jilm[(j, i, l, m)] = count_jilm[(j, i, l, m)] / count_ilm[(i, l, m)]

# MODEL 1 DIRECTION 2

sys.stderr.write("Training with IBM Model 1 in direction 2............\n")
# IBM Model 1 pseudocode obtained from http://mt-class.org/jhu/slides/lecture-ibm-model1.pdf

bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(e_data), open(f_data))[:opts.num_sents]]
t_ef = {}
for (n, (e, f)) in enumerate(bitext):
  for e_i in set(e):
    for f_j in set(f):
      t_ef[(e_i, f_j)] = 0.25 # uniform initialization

for iteration in range(0, opts.num_iters):
  sys.stderr.write("Iteration " + str(iteration) + "\n")
  sys.stderr.write("Initializing counts\n")
  # initialize counts and totals
  count_ef = {}
  total_f = {}
  for (e_i, f_j) in t_ef.keys():
    count_ef[(e_i, f_j)] = 0
    total_f[f_j] = 0

  for (n, (e, f)) in enumerate(bitext):
    # sys.stderr.write("Sentence " + str(n) + "\n")
    # compute normalization
    s_total = {}
    for e_i in set(e):
      s_total[e_i] = 0
      for f_j in set(f):
        s_total[e_i] += t_ef[(e_i, f_j)]

    # collect counts
    for e_i in set(e):
      for f_j in set(f):
        count_ef[(e_i, f_j)] += ( t_ef[(e_i, f_j)] / s_total[e_i] )
        total_f[f_j] += ( t_ef[(e_i, f_j)] / s_total[e_i] )

  # estimate probabilities
  sys.stderr.write("Estimating probabilities\n")
  for (e_i, f_j) in t_ef.keys():
    t_ef[(e_i, f_j)] = count_ef[(e_i, f_j)] / total_f[f_j]

# MODEL 2 DIRECTION 2

sys.stderr.write("Training model with IBM Model 2 in direction 2............\n")
# IBM Model 2 pseudocode obtained from http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/ibm12.pdf

q_jilm2 = {}
for (n, (e, f)) in enumerate(bitext):
  for (i, e_i) in enumerate(e):
    for (j, f_j) in enumerate(f):
      l = len(f)
      m = len(e)
      q_jilm2[(j, i, l, m)] = 0.25 # uniform initialization

for iteration in range(0, opts.num_iters):
  sys.stderr.write("Iteration " + str(iteration) + "\n")
  sys.stderr.write("Initializing counts\n")
  # initialize counts and totals
  count_ef = {}
  count_f = {}
  count_jilm = {}
  count_ilm = {}
  for (e_i, f_j) in t_ef.keys():
    count_ef[(e_i, f_j)] = 0
    count_f[f_j] = 0
  for (j, i, l, m) in q_jilm2.keys():
    count_jilm[(j, i, l, m)] = 0
    count_ilm[(i, l, m)] = 0

  for (n, (e, f)) in enumerate(bitext):
    # sys.stderr.write("Sentence " + str(n) + "\n")
    # compute normalization
    s_total = {}
    for (i, e_i) in enumerate(e):
      s_total[i] = 0
      for (j, f_j) in enumerate(f):
        s_total[i] += ( q_jilm2[(j, i, len(f), len(e))] * t_ef[(e_i, f_j)] )

    # collect counts
    for (i, e_i) in enumerate(e):
      for (j, f_j) in enumerate(f):
        delta = ( q_jilm2[(j, i, len(f), len(e))] * t_ef[(e_i, f_j)] ) / s_total[i]
        count_ef[(e_i, f_j)] += delta
        count_f[f_j] += delta
        count_jilm[(j, i, len(f), len(e))] += delta
        count_ilm[(i, len(f), len(e))] += delta

  # estimate probabilities
  sys.stderr.write("Estimating probabilities\n")
  for (e_i, f_j) in t_ef.keys():
    t_ef[(e_i, f_j)] = count_ef[(e_i, f_j)] / count_f[f_j]
  for (j, i, l, m) in q_jilm2.keys():
    q_jilm2[(j, i, l, m)] = count_jilm[(j, i, l, m)] / count_ilm[(i, l, m)]


sys.stderr.write("Generating alignments...........\n")
# generate optimal alignments
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]
for (f, e) in bitext:
  for (i, f_i) in enumerate(f): 
    best_prob = 0
    best_j = 0
    for (j, e_j) in enumerate(e):
      if (t_fe[(f_i, e_j)] * q_jilm[(j, i, len(e), len(f))]) + (t_ef[(e_j, f_i)] * q_jilm2[(i, j, len(f), len(e))]) > best_prob:
        best_prob = (t_fe[(f_i, e_j)] * q_jilm[(j, i, len(e), len(f))]) + (t_ef[(e_j, f_i)] * q_jilm2[(i, j, len(f), len(e))])
        best_j = j
    sys.stdout.write("%i-%i " % (i, best_j))
  sys.stdout.write("\n")

